{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sbs\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "class Sine_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sine_Model, self).__init__()\n",
    "        self.lin1 = nn.Linear(1, 128)\n",
    "        self.lin2 = nn.Linear(128, 128)\n",
    "        self.lin3 = nn.Linear(128, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.lin1(x.view(-1, 1)))\n",
    "        x = self.dropout(torch.tanh(self.lin2(x)))\n",
    "        return torch.tanh(self.lin3(torch.tanh(x))).squeeze()\n",
    "    \n",
    "class SineWaveTask:\n",
    "    def __init__(self, train_size):\n",
    "        self.a = np.random.uniform(0.5, 1.0)\n",
    "        self.b = np.random.uniform(0, 2)\n",
    "        self.train_x, self.train_y = self.training_set(size = train_size)\n",
    "        self.test_x,  self.test_y  = self.test_set()\n",
    "        \n",
    "    def f(self, x):\n",
    "        return self.a * np.sin(x + self.b * np.pi)\n",
    "        \n",
    "    def training_set(self, size=10):\n",
    "        x = np.random.uniform(-5, 5, size)\n",
    "        y = self.f(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "    \n",
    "    def test_set(self, size=50):\n",
    "        x = np.linspace(-10, 10, size)\n",
    "        y = self.f(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "    \n",
    "    def plot(self, *args, **kwargs):\n",
    "        plt.plot(self.test_x.numpy(),  self.test_y.numpy(), 'o-',  label = 'Test', color='blue')\n",
    "        plt.scatter(self.train_x.numpy(), self.train_y.numpy(),   label = 'Train', color='black')\n",
    "def replace_grad(parameter_gradients, parameter_name):\n",
    "    \"\"\"Creates a backward hook function that replaces the calculated gradient\n",
    "    with a precomputed value when .backward() is called.\n",
    "    \n",
    "    See\n",
    "    https://pytorch.org/docs/stable/autograd.html?highlight=hook#torch.Tensor.register_hook\n",
    "    for more info\n",
    "    \"\"\"\n",
    "    def replace_grad_(module):\n",
    "        return parameter_gradients[parameter_name]\n",
    "\n",
    "    return replace_grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tasks = [SineWaveTask(train_size = 50) for i in range(100)]\n",
    "target_task  = SineWaveTask(train_size = 50)\n",
    "\n",
    "for task in source_tasks:\n",
    "    task.plot()\n",
    "plt.show()  \n",
    "\n",
    "target_task.plot()\n",
    "plt.show()\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = []\n",
    "model = Sine_Model().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "# Transfer Learning (Learning all the tasks simultaneously with a single model, fail)\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-3)\n",
    "for epoch in np.arange(10) + 1:\n",
    "    for out_batch in np.arange(128):\n",
    "        for task in source_tasks + [target_task]:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            x, y = task.train_x.to(device), Variable(task.train_x).to(device)\n",
    "            pred_y = model.forward(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        x, y = target_task.test_x.to(device), target_task.test_y.to(device)\n",
    "        pred_y = model.forward(x)\n",
    "        l = criterion(pred_y, y)\n",
    "        test_loss += [l.cpu().detach().numpy()]\n",
    "    plt.title(l.cpu().detach().numpy())\n",
    "    plt.plot(x.cpu().detach().numpy(), pred_y.cpu().detach().numpy(), '^--', label='predict')\n",
    "    target_task.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plt.plot(test_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = []\n",
    "model = Sine_Model().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "# Supervise\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-3)\n",
    "for epoch in np.arange(10) + 1:\n",
    "    for batch in np.arange(128):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x, y = target_task.train_x.to(device), Variable(target_task.train_y).to(device)\n",
    "        pred_y = model.forward(x)\n",
    "        loss = criterion(pred_y, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        x, y = target_task.test_x.to(device), target_task.test_y.to(device)\n",
    "        pred_y = model.forward(x)\n",
    "        l = criterion(pred_y, y)\n",
    "        test_loss += [l.cpu().detach().numpy()]\n",
    "    plt.title(l.cpu().detach().numpy())\n",
    "    plt.plot(x.cpu().detach().numpy(), pred_y.cpu().detach().numpy(), '^--', label='predict')\n",
    "    target_task.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plt.plot(test_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_loss = []\n",
    "meta_test_loss  = []\n",
    "lr_inner = 0.01\n",
    "model     = Sine_Model().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "# MAML\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "for epoch in np.arange(10) + 1:\n",
    "    for meta_batch in np.arange(128):\n",
    "        task_grads = []\n",
    "        task_losss = []\n",
    "        '''\n",
    "        Cumulate Inner Gradient\n",
    "        '''\n",
    "        for task in source_tasks:\n",
    "            model_tmp = copy.deepcopy(model)\n",
    "            model_tmp.train()\n",
    "            optimizer_tmp = torch.optim.Adam(model.parameters())\n",
    "            for inner_batch in range(4):\n",
    "                optimizer_tmp.zero_grad()\n",
    "                x, y = task.train_x.to(device), Variable(task.train_y).to(device)\n",
    "                pred_y = model_tmp.forward(x)\n",
    "                loss = criterion(pred_y, y)\n",
    "                loss.backward()\n",
    "                optimizer_tmp.step()\n",
    "            model_tmp.eval()\n",
    "            optimizer_tmp.zero_grad()\n",
    "            x, y = task.test_x.to(device), Variable(task.test_y).to(device)\n",
    "            pred_y = model_tmp.forward(x)\n",
    "            loss = criterion(pred_y, y)   \n",
    "            task_losss += [loss.cpu().detach().numpy()]\n",
    "            loss.backward()\n",
    "            task_grads += [{name: param.grad for (name, param) in model_tmp.named_parameters()}]\n",
    "        meta_train_loss += [np.average(task_losss)]\n",
    "        '''\n",
    "        Evaluate:\n",
    "        '''\n",
    "        model_tmp = copy.deepcopy(model)\n",
    "        model_tmp.train()\n",
    "        optimizer_tmp = torch.optim.Adam(model_tmp.parameters())\n",
    "        for inner_batch in range(4):\n",
    "            optimizer_tmp.zero_grad()\n",
    "            x, y = target_task.train_x.to(device), Variable(target_task.train_y).to(device)\n",
    "            pred_y = model_tmp.forward(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            loss.backward()\n",
    "            optimizer_tmp.step()\n",
    "        model_tmp.eval()\n",
    "        x, y = target_task.test_x.to(device), Variable(target_task.test_y).to(device)\n",
    "        pred_target = model_tmp.forward(x)\n",
    "        loss = criterion(pred_target, y)   \n",
    "        meta_test_loss += [loss.cpu().detach().numpy()]  \n",
    "        \n",
    "        '''\n",
    "        Meta-Update\n",
    "        '''\n",
    "        \n",
    "        avg_task_grad = {name: torch.stack([name_grad[name] for name_grad in task_grads]).mean(dim=0)\n",
    "                                  for name in task_grads[0].keys()}\n",
    "        \n",
    "        hooks = []\n",
    "        for name, param in model.named_parameters():\n",
    "            hooks.append(\n",
    "                param.register_hook(replace_grad(avg_task_grad, name))\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        pred_y = model.forward(x)\n",
    "        loss = criterion(pred_y, y)\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        \n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "    plt.plot(meta_train_loss, label = 'meta_train')\n",
    "    plt.plot(meta_test_loss,  label = 'meta_test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(target_task.test_x.cpu().detach().numpy(), pred_y.cpu().detach().numpy(), '^--', label='predict')\n",
    "    target_task.plot()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
